### **Краткое описание параметров PPO (Stable Baselines3)**

1. **Основные параметры**  
   - `policy` – тип политики (например, `"MlpPolicy"` для полносвязной сети).  
   - `env` – среда обучения (объект Gymnasium или имя строкой).  
   - `learning_rate` Скорость обучения (может быть функцией для адаптивного изменения). От 1e-5 (0.00001) до 1e-2 (0.01).
      <details>
      <summary>Подробнее</summary>  

      Контролирует размер шага обновления

      - Слишком высокий learning_rate → обновления будут слишком большими, обучение станет нестабильным (может "прыгать" вокруг оптимума или расходиться).

      - Слишком низкий learning_rate → обучение будет медленным, модель может застрять в локальном минимуме. 
      </details>

2. **Параметры обучения**  
   - `n_steps` – количество шагов в эпизоде перед обновлением модели.
      <details>
      <summary>Подробнее</summary>  
      Количество шагов обучения (итераций обновления весов). Обычно от 100 до 100000+ в зависимости от сложности задачи.

      Определяет продолжительность обучения модели:

      - Слишком мало <code>n_steps</code> → модель не успеет сойтись (недообучение), результаты будут неточными.
      - Слишком много <code>n_steps</code> → возможен переобучение (модель "запомнит" тренировочные данные), трата вычислительных ресурсов.

      <strong>Рекомендации:</strong>
      - Начинайте с 1000-5000 шагов для простых задач
      - Используйте раннюю остановку (early stopping) для автоматического определения оптимального числа шагов
      - Для сложных моделей (например, нейросетей) могут потребоваться десятки/сотни тысяч шагов
      </details>

   - `batch_size` – размер батча для обновления.  
   - `n_epochs` – количество проходов по данным за одну итерацию.  
   - `gamma` – коэффициент дисконтирования (0.99 = сильное влияние будущих наград). 
      <details>
      <summary>Подробнее</summary>

      **Коэффициент дисконтирования** (discount factor). Диапазон: от 0 до 1.

      - `gamma = 0`: только текущие награды
      - `gamma → 1`: долгосрочное планирование

      </details> 

   - `gae_lambda` – параметр для Generalized Advantage Estimation (оптимизация оценки преимущества).  

3. **Оптимизация и регуляризация**  
   - `clip_range` – ограничение для обновления политики (0.2 = ±20% изменение).  
   - `clip_range_vf` – аналогично для функции ценности (если `None`, равен `clip_range`).  
   - `normalize_advantage` – нормализовать преимущества (улучшает стабильность).  
   - `ent_coef` – коэффициент энтропии (поощрение исследования, например, 0.01).
      <details>
      <summary>
         Подробнее...
      </summary>
      Коэффициент энтропии (entropy coefficient), используемый в алгоритмах RL (например, PPO) для регулирования исследования. Обычно небольшое положительное значение (0.01-0.1).

      <strong>Назначение:</strong>
      - Поощряет политику за исследование новых действий
      - Предотвращает преждевременную сходимость к субоптимальной стратегии
      - Контролирует баланс между исследованием и эксплуатацией

      <strong>Эффекты:</strong>
      - <strong>Высокий <code>ent_coef</code></strong> → агент больше исследует, но может хуже использовать найденные хорошие стратегии
      - <strong>Низкий <code>ent_coef</code></strong> → агент быстро сходится, но может застрять в локальном оптимуме
      - <strong><code>ent_coef = 0</code></strong> → полностью отключает энтропийное поощрение

      <strong>Рекомендации:</strong>
      - Начинайте с 0.01 для большинства задач
      - Увеличивайте (0.05-0.1) для сред с редкими наградами
      - Уменьшайте (0.001-0.01) для детерминированных сред
      - Можно адаптивно изменять в ходе обучения

      <em>Математически:</em> добавляет к функции потерь член:<br>
      <code>L = ... + β·H(π(a|s))</code><br>
      где H - энтропия политики, β - <code>ent_coef</code>
      </details>  
   - `vf_coef` – вес функции ценности в loss-функции (0.5 = баланс с политикой).  
   - `max_grad_norm` – ограничение градиентов (предотвращает "взрыв" градиентов).  

4. **Дополнительные настройки**  
   - `use_sde` – использовать стохастическую политику (для непрерывных действий).  
   - `sde_sample_freq` – частота обновления шума для `use_sde`.  
   - `target_kl` – целевое значение KL-дивергенции (если превышено, обучение останавливается).  
   - `stats_window_size` – размер окна для усреднения статистики (например, 100 эпизодов).  

5. **Логирование и аппаратура**  
   - `tensorboard_log` – путь для сохранения логов TensorBoard.  
   - `verbose` – уровень детализации вывода (0 = нет, 1 = информация).  
   - `seed` – seed для воспроизводимости.  
   - `device` – устройство для обучения (`"cpu"`, `"cuda"`, или `"auto"`).  

---

### **Советы по настройке**  
- Для непрерывных действий: `policy="MlpPolicy"`, `use_sde=True`.  
- Для дискретных: `policy="CnnPolicy"` (если есть изображения).  
- Уменьшите `clip_range` (например, до 0.1) для более стабильного обучения.  
- Если среда сложная, попробуйте увеличить `n_steps` и `batch_size`.  
