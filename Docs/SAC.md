### **`policy` (Политика)**

*   **Перевод:** Модель политики, которую будет использовать агент (MlpPolicy, CnnPolicy, ...).
*   **Объяснение простыми словами:** Это "мозг" вашего агента. Политика решает, какое действие совершить в текущей ситуации.
    *   **`MlpPolicy`** (Multi-Layer Perceptron): Стандартный мозг для данных в виде чисел (например, координаты, скорость, углы). Подходит для большинства задач, где на вход подается не картинка, а вектор данных.
    *   **`CnnPolicy`** (Convolutional Neural Network): Специализированный мозг для обработки изображений. Используйте его, если ваш агент "видит" мир через пиксели (например, играет в игру Atari).
*   **Рекомендация:** Библиотека обычно сама выбирает правильный тип политики в зависимости от среды. Вам редко нужно менять этот параметр вручную.

---

### **`env` (Среда)**

*   **Перевод:** Среда, в которой агент будет обучаться.
*   **Объяснение простыми словами:** Это "мир" или "игра", в которой действует ваш агент. Например, симулятор робота, который учится ходить, или видеоигра. Это не столько параметр для настройки, сколько сама задача, которую вы решаете.

---

### **`learning_rate` (Скорость обучения)**

*   **Перевод:** Скорость обучения для оптимизатора Adam.
*   **Объяснение простыми словами:** Представьте, что вы спускаетесь с горы в тумане и хотите найти самую низкую точку (минимум ошибки). `learning_rate` — это размер ваших шагов.
    *   **Слишком большой шаг:** Вы можете перепрыгнуть самую низкую точку и никогда ее не найти. Обучение будет нестабильным.
    *   **Слишком маленький шаг:** Вы будете двигаться очень медленно, и обучение займет вечность.
*   **Рекомендация:** Значение по умолчанию (часто `3e-4` или `0.0003`) — хорошая отправная точка. Если обучение расходится (награды скачут вверх-вниз без улучшений), попробуйте уменьшить скорость обучения (например, до `1e-4`).

---

### **`buffer_size` (Размер буфера)**

*   **Перевод:** Размер буфера воспроизведения (replay buffer).
*   **Объяснение простыми словами:** SAC — это алгоритм, который учится на своем прошлом опыте. Он хранит этот опыт (ситуация, действие, награда, новая ситуация) в специальной "памяти", которая называется буфер воспроизведения. `buffer_size` — это максимальное количество воспоминаний, которые агент может хранить.
*   **Аналогия:** Это как записная книжка. Агент постоянно записывает в нее свой опыт, а когда она заполняется, стирает самые старые записи. Для обучения он случайным образом просматривает страницы из этой книжки.
*   **Рекомендация:** Чем больше буфер, тем разнообразнее опыт, на котором учится агент, что обычно хорошо. Стандартное значение — `1_000_000`. Главное ограничение — объем оперативной памяти (RAM) на вашем компьютере.

---

### **`learning_starts` (Начало обучения)**

*   **Перевод:** Через сколько шагов в среде агент начнет обучение.
*   **Объяснение простыми словами:** В самом начале у агента нет никакого опыта (его "записная книжка" пуста). Бессмысленно пытаться чему-то научиться на пустом месте. Этот параметр говорит агенту: "Сначала просто действуй случайно в течение `X` шагов, заполни свою память начальным опытом, и только потом начинай анализировать его и учиться".
*   **Рекомендация:** Значение должно быть хотя бы равно `batch_size`. Обычно ставят `1000` или `10000`, чтобы накопить достаточно разнообразных данных перед первым обновлением.

---

### **`batch_size` (Размер пакета/батча)**

*   **Перевод:** Размер мини-пакета для каждого шага градиентного спуска.
*   **Объяснение простыми словами:** Когда агент учится, он не просматривает всю свою память (`buffer_size`) сразу — это слишком долго. Вместо этого он берет случайную выборку из памяти — "пакет" или "батч". `batch_size` — это размер этой выборки.
*   **Аналогия:** Вместо того чтобы перечитывать весь учебник перед экзаменом, вы просматриваете несколько случайных страниц.
*   **Рекомендация:** `256` — популярное и хорошее значение по умолчанию. Большие батчи делают обучение более стабильным, но медленным. Маленькие — наоборот.

---

### **`tau` (Коэффициент мягкого обновления)**

*   **Перевод:** Коэффициент мягкого обновления (обновление Поляка).
*   **Объяснение простыми словами:** Для стабильности обучения у SAC есть две версии сетей: основная (которая активно учится) и целевая (стабильная, более старая копия). Вместо того чтобы резко копировать веса из основной сети в целевую, мы их "плавно переливаем". `tau` контролирует, какую долю новой информации мы "переливаем" за один раз.
    *   `tau = 1`: Резкое копирование (обычно плохо).
    *   `tau = 0.005`: Мы берем 99.5% от старой целевой сети и добавляем 0.5% от новой основной сети. Это делает целевую сеть очень стабильной.
*   **Рекомендация:** Всегда используйте маленькое значение, например, `0.005` (по умолчанию) или `0.01`. Менять его стоит только в крайних случаях.

---

### **`gamma` (Коэффициент дисконтирования)**

*   **Перевод:** Коэффициент дисконтирования будущих наград.
*   **Объяснение простыми словами:** Этот параметр определяет, насколько агент "дальнозорок". Он отвечает на вопрос: "Награда, полученная прямо сейчас, важнее награды, которую я получу в будущем?"
    *   `gamma` близко к 1 (например, `0.99`): Агент очень ценит будущие награды. Он готов пожертвовать сиюминутной выгодой ради большей награды в будущем (стратегическое мышление).
    *   `gamma` близко к 0: Агент "живет одним днем" и заботится только о немедленной награде.
*   **Рекомендация:** Для большинства задач используют `0.99`. Если эпизоды в вашей среде очень короткие, можно использовать значение поменьше, например, `0.95`.

---

### **`train_freq` (Частота обучения)**

*   **Перевод:** Как часто обновлять модель.
*   **Объяснение простыми словами:** Этот параметр определяет, когда запускать процесс обучения. Например, `(1, "step")` означает, что после каждого *одного шага* в среде агент будет выполнять обновление своих сетей. `(1, "episode")` означает, что обучение будет происходить только в конце каждого эпизода.
*   **Рекомендация:** Для SAC и других "off-policy" алгоритмов почти всегда используется `(1, "step")`, что позволяет максимально эффективно использовать собранные данные.

---

### **`gradient_steps` (Количество шагов градиента)**

*   **Перевод:** Сколько шагов градиентного спуска делать после сбора данных.
*   **Объяснение простыми словами:** Когда наступает момент обучения (согласно `train_freq`), сколько раз мы будем брать батч из памяти и обновлять сети? Если `gradient_steps = 10`, то после одного шага в среде агент 10 раз извлечет случайный батч из памяти и обновит свои "мозги". Это позволяет лучше "переварить" полученный опыт.
*   **Рекомендация:** `1` — стандартное значение. Если ваша среда очень медленная (например, сложный симулятор), а вычисления на GPU быстрые, можно увеличить это значение, чтобы ускорить обучение. `-1` означает, что будет сделано столько же шагов градиента, сколько шагов в среде (при `train_freq = (1, "step")` это эквивалентно `1`).

---

### **`ent_coef` (Коэффициент энтропии)**

*   **Перевод:** Коэффициент регуляризации энтропии.
*   **Объяснение простыми словами:** Это **ключевой параметр SAC**. Энтропия — это мера "случайности" или "непредсказуемости" действий агента.
    *   **Высокий `ent_coef`:** Агент получает дополнительную награду за то, что ведет себя максимально случайно. Это поощряет **исследование (exploration)** — пробовать новые, нестандартные действия.
    *   **Низкий `ent_coef`:** Агент фокусируется только на основной награде и использует уже известные, проверенные действия. Это поощряет **эксплуатацию (exploitation)**.
    *   **`'auto'`:** Это главная "фишка" SAC! Алгоритм сам подбирает оптимальный баланс между исследованием и эксплуатацией, обучаясь этому коэффициенту.
*   **Рекомендация:** **Всегда используйте `'auto'`.** Это одна из причин, почему SAC так хорошо работает. `auto_0.1` просто задает начальное значение для этого коэффициента.

---

### **`target_entropy` (Целевая энтропия)**

*   **Перевод:** Целевая энтропия при обучении `ent_coef`.
*   **Объяснение простыми словами:** Когда вы используете `ent_coef = 'auto'`, алгоритму нужна цель — к какому уровню "случайности" стремиться. Этот параметр задает эту цель.
    *   `'auto'`: Библиотека сама вычисляет разумное значение по умолчанию (обычно `-размерность_пространства_действий`).
    *   Число (float): Вы можете задать его вручную. Более низкое значение (например, `-10`) заставит агента быть более детерминированным, высокое (например, `-1`) — более случайным.
*   **Рекомендация:** Оставьте `'auto'`. Библиотека отлично справляется с подбором этого значения.

---

### **Остальные параметры (более специфичные):**

*   **`action_noise` (Шум действий):** Устаревший для SAC способ исследования. SAC использует энтропию, поэтому здесь почти всегда стоит `None`.
*   **`replay_buffer_class` и `replay_buffer_kwargs`:** Для продвинутых пользователей, которые хотят использовать свой собственный класс буфера памяти (например, для Hindsight Experience Replay). В 99% случаев это не нужно.
*   **`optimize_memory_usage` (Оптимизация использования памяти):** Экономит RAM, но может немного замедлить работу. Включайте, если у вас не хватает памяти для `buffer_size`.
*   **`target_update_interval` (Интервал обновления целевой сети):** Как часто выполнять мягкое обновление `tau`. По умолчанию `1`, т.е. на каждом шаге градиента. Не стоит менять.
*   **`use_sde` (Использовать State-Dependent Exploration):** Более продвинутый метод исследования, чем просто шум. Может быть эффективен в сложных средах. Стоит попробовать, если стандартный SAC застревает.
*   **`sde_sample_freq` (Частота сэмплирования SDE):** Как часто пересчитывать матрицу шума при использовании `use_sde`. `-1` (по умолчанию) означает, что шум сэмплируется один раз в начале каждого эпизода.