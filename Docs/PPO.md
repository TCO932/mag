**Главное отличие от SAC:** PPO учится только на "свежем" опыте. Он собирает порцию данных, учится на ней, обновляет свою политику, а затем **выбрасывает эти данные** и собирает новую порцию с помощью уже обновленной политики. У него нет огромного "буфера воспроизведения" (`replay_buffer`), как у SAC.

Давайте разберем его параметры.

---

### **`policy` (Политика)**

*   **Перевод:** Модель политики (MlpPolicy, CnnPolicy, ...).
*   **Объяснение:** То же самое, что и в SAC. Это "мозг" агента. `MlpPolicy` для числовых данных, `CnnPolicy` для картинок. В PPO используется тип `ActorCriticPolicy`, потому что у PPO есть две отдельные (или совмещенные) головы: одна решает, *что делать* (Actor), а вторая оценивает, *насколько хороша текущая ситуация* (Critic).

---

### **`env` (Среда)**

*   **Перевод:** Среда для обучения.
*   **Объяснение:** Аналогично SAC, это мир, в котором действует агент.

---

### **`learning_rate` (Скорость обучения)**

*   **Перевод:** Скорость обучения.
*   **Объяснение:** Так же, как и в SAC, это размер шага при обновлении нейросети.
*   **Рекомендация:** Стандартное значение `3e-4` (0.0003) — отличная отправная точка.

---

### **`n_steps` (Количество шагов для сбора данных)**

*   **Перевод:** Количество шагов, которые нужно выполнить в среде перед каждым обновлением.
*   **Объяснение:** Это один из **ключевых параметров PPO**. Агент играет в среде ровно `n_steps` шагов, собирая опыт (состояние, действие, награда). После этого он останавливается и использует всю собранную порцию данных для своего обучения.
*   **Аналогия:** Это как "рабочая смена" для агента. Он работает `n_steps` часов, а потом идет на "тренинг", чтобы проанализировать свою работу. После тренинга он начинает новую смену с новыми знаниями.
*   **Рекомендация:** Это компромисс.
    *   **Большое значение (e.g., 2048, 4096):** Более стабильные обновления, но реже. Агент дольше собирает данные и реже обновляет свою стратегию. Это стандарт для PPO.
    *   **Маленькое значение (e.g., 128, 256):** Более частые, но менее стабильные обновления.
    *   **Начните с `2048`.**

---

### **`batch_size` (Размер мини-пакета)**

*   **Перевод:** Размер мини-пакета/батча.
*   **Объяснение:** Когда агент собрал свой опыт за `n_steps`, он не обрабатывает его весь сразу. Он делит этот большой кусок данных на маленькие "мини-пакеты" размером `batch_size` и обучается на них по очереди.
*   **Важно:** `batch_size` должен быть меньше, чем `n_steps`.
*   **Рекомендация:** Стандартное значение `64` — хорошая отправная точка.

---

### **`n_epochs` (Количество эпох)**

*   **Перевод:** Количество эпох при оптимизации.
*   **Объяснение:** Этот параметр говорит, сколько раз нужно "пройтись" по всему собранному опыту (`n_steps`) во время одной фазы обучения.
*   **Аналогия:** Если `n_steps` — это отчет о вашей "рабочей смене", то `n_epochs` — это сколько раз вы его перечитаете, чтобы усвоить информацию, прежде чем начать новую смену.
*   **Рекомендация:** Стандартное значение `10`. Слишком большое значение может привести к переобучению на этой конкретной порции данных.

---

### **`gamma` (Коэффициент дисконтирования)**

*   **Перевод:** Коэффициент дисконтирования.
*   **Объяснение:** То же самое, что и в SAC. Определяет "дальнозоркость" агента.
*   **Рекомендация:** `0.99` — почти всегда лучший выбор для PPO.

---

### **`gae_lambda` (Лямбда для GAE)**

*   **Перевод:** Фактор для компромисса между смещением и дисперсией в Generalized Advantage Estimator (GAE).
*   **Объяснение простыми словами:** GAE — это умный способ оценить, насколько хорошим было действие агента. `lambda` контролирует, насколько "далеко в будущее" мы смотрим при этой оценке.
    *   `lambda = 0`: Оцениваем действие только по следующему шагу (высокое смещение, низкая дисперсия).
    *   `lambda = 1`: Оцениваем действие по всей оставшейся траектории до конца эпизода (низкое смещение, высокая дисперсия).
*   **Аналогия:** Это как оценивать футболиста. `lambda=0`: мы смотрим только на то, удачно ли он отдал пас. `lambda=1`: мы смотрим, привела ли в итоге его игра к голу. `lambda=0.95` (стандарт) — золотая середина: мы смотрим и на качество паса, и на то, как он повлиял на игру в ближайшие несколько секунд.
*   **Рекомендация:** **Не трогайте этот параметр.** `0.95` — это "магическое" число, которое отлично работает почти всегда.

---

### **`clip_range` (Диапазон отсечения)**

*   **Перевод:** Параметр отсечения (клиппинга).
*   **Объяснение:** **ЭТО СЕРДЦЕ АЛГОРИТМА PPO!** "P" в PPO означает "Proximal" (близкий). Алгоритм боится делать слишком большие и резкие обновления, которые могут всё испортить. `clip_range` создает "коридор" или "заборчик" вокруг текущей политики. Новая политика не может слишком сильно отличаться от старой за один шаг обновления.
*   **Аналогия:** Вы настраиваете очень чувствительный прибор. Вместо того чтобы крутить ручку как попало, вы делаете только небольшие, осторожные повороты. `clip_range=0.2` означает, что вы не можете повернуть ручку больше, чем на 20% от её текущего положения.
*   **Рекомендация:** `0.2` — стандартное и очень хорошее значение.

---

### **Другие важные параметры**

*   **`vf_coef` (Коэффициент функции ценности):** Насколько важна ошибка "критика" (Value Function) в общей функции потерь. `0.5` — стандарт.
*   **`ent_coef` (Коэффициент энтропии):** Как и в SAC, поощряет исследование. В PPO он не настраивается автоматически. Если агент быстро застревает в одном и том же поведении, можно немного увеличить этот коэффициент (например, до `0.01`).
*   **`max_grad_norm` (Максимальная норма градиента):** Еще один механизм стабильности. Если обновление получается "взрывным", этот параметр его "обрезает" до приемлемого размера. Аналог автоматического выключателя в электросети. `0.5` — стандарт.
*   **`normalize_advantage` (Нормализация преимущества):** Простой трюк, который почти всегда улучшает стабильность. Он нормализует оценки действий перед их использованием.
*   **Рекомендация для этих параметров:** Оставьте значения по умолчанию. Они подобраны очень хорошо.

---

### Вспомогательные и продвинутые параметры

*   **`clip_range_vf`:** То же самое, что `clip_range`, но для "критика". Обычно не используется (`None`).
*   **`use_sde`, `sde_sample_freq`:** Для продвинутого исследования (State-Dependent Exploration), как и в SAC.
*   **`target_kl`:** Еще один способ ограничить слишком большие обновления, но через KL-дивергенцию. Обычно не используется.
*   **`tensorboard_log`, `policy_kwargs`, `verbose`, `seed`, `device`:** Параметры для логирования, настройки "мозга", вывода информации, воспроизводимости и выбора устройства (CPU/GPU). Аналогичны параметрам в SAC.

### Итог: PPO vs SAC в терминах параметров

| Концепция                      | PPO                                               | SAC                                                          |
| ------------------------------ | ------------------------------------------------- | ------------------------------------------------------------ |
| **Тип алгоритма**              | On-Policy (учится на свежих данных)               | Off-Policy (учится на старых данных из буфера)               |
| **Сбор данных**                | Собирает `n_steps` данных, потом учится.          | Постоянно добавляет данные в `buffer_size` и учится на них. |
| **Использование данных**       | Использует данные один раз (в течение `n_epochs`), потом выбрасывает. | Многократно использует данные из буфера.                     |
| **Ключевой параметр "новизны"** | **`clip_range`** (не дает политике сильно меняться) | **`tau`** (плавно обновляет целевую сеть)                     |
| **Ключевой параметр исследования** | `ent_coef` (фиксированный)                        | `ent_coef='auto'` (автоматически настраиваемый)               |