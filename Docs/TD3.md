### Философия TD3: Борьба с "Оптимизмом"

Чтобы понять TD3, нужно знать его предшественника, DDPG. DDPG был прорывным, но страдал от одной фундаментальной проблемы: **переоценка Q-значений (Overestimation Bias)**.

Представьте, что вы оцениваете стоимость разных домов (действий). DDPG был как слишком оптимистичный риэлтор, который всегда завышал цену. Обучаясь на этих завышенных оценках, агент начинал выбирать действия, которые *казались* отличными, но на самом деле были плохими. Это приводило к катастрофически нестабильному обучению.

**TD3 — это DDPG, на который надели "костюм инженера по безопасности".** Он вводит три механизма, чтобы systematically бороться с этим губительным оптимизмом и стабилизировать обучение. Каждый из ключевых параметров TD3 — это часть одного из этих механизмов.

---

### Группа 1: Фундаментальные параметры (Общие для Off-Policy алгоритмов)

Эти параметры идентичны SAC, так как оба алгоритма используют буфер воспроизведения.

*   **`policy` (MlpPolicy, CnnPolicy, ...):** "Мозг" агента. `MlpPolicy` для векторов (координаты, скорости), `CnnPolicy` для пикселей. Здесь ничего нового.

*   **`env`:** Ваш "спортзал" для агента.

*   **`learning_rate` (Скорость обучения):**
    *   **Что это?** Размер шага, которым нейросети обновляют свои веса.
    *   **Аналогия:** Размер шага при спуске с горы в тумане.
    *   **Детали:** TD3 имеет три сети (два Критика и один Актер) и их целевые копии. Эта скорость обучения применяется ко всем трем основным сетям. Слишком высокая скорость может привести к "взрывам" и расхождению обучения, слишком низкая — к вечному обучению. `3e-4` (0.0003) — это проверенная временем, надежная отправная точка.

*   **`buffer_size` (Размер буфера воспроизведения):**
    *   **Что это?** Максимальное количество "воспоминаний" (опыта `[состояние, действие, награда, новое состояние]`), которые агент хранит.
    *   **Детали:** Большой буфер (`1_000_000`) позволяет агенту учиться на очень разнообразном опыте, что снижает корреляцию между последовательными обновлениями и делает обучение стабильнее. Главный ограничитель — ваша оперативная память (RAM).

*   **`learning_starts`:**
    *   **Что это?** "Карантин" для агента. Он должен сначала собрать `learning_starts` шагов случайного опыта, прежде чем ему разрешат учиться.
    *   **Детали:** Начинать учиться на пустом или очень маленьком буфере бессмысленно. Агент выучит только то, как он падает в первые секунды. Нужно накопить разнообразный опыт. `10_000` шагов — хороший минимум, чтобы буфер наполнился разными сценариями.

*   **`batch_size` (Размер пакета):**
    *   **Что это?** Количество "воспоминаний", которые агент извлекает из буфера за один шаг градиентного спуска.
    *   **Детали:** Стандарт `256`. Большие батчи дают более стабильную оценку градиента, но каждый шаг обучения дольше. Меньшие батчи "шумнее", но позволяют делать больше обновлений за то же время.

*   **`tau` (Коэффициент мягкого обновления):**
    *   **Что это?** Коэффициент для обновления "замороженных" целевых сетей.
    *   **Детали:** Формула обновления: `целевая_сеть = tau * основная_сеть + (1 - tau) * целевая_сеть`. При `tau=0.005`, мы берем 99.5% старой целевой сети и добавляем всего 0.5% информации от основной, активно обучающейся сети. Это делает цели для обучения очень стабильными, что является ключевым фактором успеха в off-policy алгоритмах.

*   **`gamma` (Коэффициент дисконтирования):**
    *   **Что это?** Определяет, насколько важны будущие награды по сравнению с текущими.
    *   **Детали:** `gamma=0.99` означает, что награда через 100 шагов будет иметь вес `0.99^100 ≈ 0.36`. Это делает агента "стратегом". Для задач, где важен долгосрочный результат (как ходьба), высокое значение `gamma` необходимо.

---

### Группа 2: Сердце TD3 (Три механизма стабильности)

Вот они, три столпа, которые делают TD3 таким надежным.

#### **Столп №1: Clipped Double Q-Learning (Двойной Критик с отсечением)**

*   **Проблема:** Одиночный критик в DDPG систематически завышает оценку будущих наград.
*   **Решение:** Использовать двух Критиков (Q-функций), обучать их независимо, а для вычисления целевого значения брать **минимальную** из их оценок.
*   **Аналогия:** Вы хотите купить дом. Вы нанимаете двух независимых оценщиков. Один — оптимист, другой — реалист. Чтобы не переплатить, вы ориентируетесь на **самую низкую** из предложенных ими цен. Это и делает TD3: он пессимист, и это защищает его от переоценки.
*   **Связанные параметры:** Эти параметры не вынесены в API, так как это фундаментальная часть архитектуры. Вы просто должны знать, что "под капотом" у TD3 всегда два критика.

#### **Столп №2: Delayed Policy Updates (Отложенные обновления Политики)**

*   **Параметр:** **`policy_delay`**
*   **Что это?** Целое число, которое говорит: "Обновляй Актера (политику) только каждый `N`-й раз, когда обновляешь Критиков".
*   **Проблема:** Даже с двумя критиками их оценки в начале обучения очень "шумные" и неточные. Если Актер будет немедленно реагировать на каждую мелкую флуктуацию в этих оценках, его политика будет "дерганой" и нестабильной.
*   **Решение:** Дать критикам время "устаканиться". Мы обновляем критиков `policy_delay` раз, позволяя их оценкам стать более точными, и только потом делаем одно обновление Актера на основе этих уже более надежных данных.
*   **Аналогия:** Генеральный директор (Актер) не меняет стратегию компании после каждого отчета от стажера-аналитика (Критик). Он ждет, пока финансовый отдел соберет и несколько раз перепроверит данные за квартал (`policy_delay` отчетов), и только потом принимает взвешенное решение.
*   **Рекомендация:** `policy_delay = 2`. Стандарт из оригинальной статьи. Это означает, что критики обновляются в два раза чаще, чем актор. Это золотая середина между скоростью и стабильностью.

#### **Столп №3: Target Policy Smoothing (Сглаживание Целевой Политики)**

*   **Параметры:** **`target_policy_noise`** и **`target_noise_clip`**
*   **Что это?**
    *   `target_policy_noise`: Стандартное отклонение шума, добавляемого к *целевому* действию.
    *   `target_noise_clip`: Максимальная величина этого шума.
*   **Проблема:** Нейросети (наши Критики) могут "схитрить". Они могут выучить очень узкий "пик" для какого-то конкретного действия, давая ему неоправданно высокую оценку. Актер может затем "вцепиться" в этот пик и эксплуатировать ошибку аппроксимации Критика.
*   **Решение:** Сделать целевые значения для Критика немного "размытыми". При расчете целевого Q-значения мы берем действие от целевого Актера, добавляем к нему небольшой случайный шум, а затем уже подаем это "зашумленное" действие в целевого Критика.
*   **Аналогия:** Вы учите лучника стрелять. Вместо того чтобы он целился в одну точку, вы вешаете мишень на веревку, и она слегка покачивается. Это заставляет его научиться целиться не в конкретный пиксель, а в небольшую область, делая его навык более общим и устойчивым. `target_policy_noise` — это сила раскачивания, а `target_noise_clip` — это ограничители, чтобы мишень не улетела совсем.
*   **Рекомендации:** `target_policy_noise = 0.2`, `target_noise_clip = 0.5`. Это стандартные значения, которые хорошо работают.

---

### Группа 3: Исследование (Критически важный аспект)

*   **Параметр:** **`action_noise`**
*   **Что это?** Механизм для исследования мира.
*   **Проблема:** Политика TD3 — **детерминированная**. Для одного и того же состояния она всегда выдаст одно и то же действие. Если не заставить ее пробовать что-то новое, она никогда не выйдет за рамки своего первоначального, случайного поведения.
*   **Решение:** На каждом шаге в среде мы берем "уверенное" действие от Актера и добавляем к нему случайный шум. Именно это "зашумленное" действие и выполняется в среде.
*   **Отличие от SAC:** SAC имеет стохастическую политику, его "мозг" сам по себе генерирует случайность (как человек, бросающий дротики с естественным разбросом). TD3 имеет детерминированную политику (как робот, всегда целящийся в одну точку). Поэтому для TD3 **внешний шум — это не опция, а необходимость.**
*   **Рекомендация:** **Обязательно использовать.** `NormalActionNoise` (Гауссовский шум) является стандартом. `sigma=0.1` — хорошая отправная точка для стандартного пространства действий, нормализованного в диапазоне `[-1, 1]`.







### Кто такой TD3? Общая идея

Представьте себе алгоритм DDPG, который был одним из первых успешных алгоритмов для непрерывного управления. У него была большая проблема: он был слишком "оптимистичен". Он часто переоценивал, насколько хороши его действия, что приводило к очень нестабильному обучению и полному провалу.

**TD3 — это DDPG, которому добавили три "стабилизатора":**

1.  **"Twin" (Двойник):** Вместо одного "критика" используются два. Для обновления выбирается более пессимистичная оценка из двух.
2.  **"Delayed" (Отложенный):** "Актер" (политика) обновляется реже, чем "критики".
3.  **Сглаживание цели (Target Smoothing):** В целевые действия добавляется шум, чтобы сделать оценку более робастной.

TD3 — это **off-policy** алгоритм, как и SAC, поэтому многие его параметры будут вам уже знакомы.

---

### Знакомые параметры (как в SAC)

Эта группа параметров работает практически идентично тому, как они работают в SAC.

*   **`policy` (MlpPolicy, CnnPolicy, ...):** "Мозг" агента.
*   **`env`:** Среда обучения.
*   **`learning_rate`:** Скорость обучения для всех сетей. `3e-4` — хороший старт.
*   **`buffer_size`:** Размер "памяти" (буфера воспроизведения). `1_000_000` — стандарт.
*   **`learning_starts`:** Сколько шагов сделать перед началом обучения. `10000` — хорошая отправная точка.
*   **`batch_size`:** Размер выборки из памяти для одного обновления. `256` — стандарт.
*   **`tau`:** Коэффициент "плавного" обновления целевых сетей. `0.005` — стандарт.
*   **`gamma`:** Коэффициент дисконтирования, "дальнозоркость" агента. `0.99` — стандарт.
*   **`train_freq`, `gradient_steps`:** Как часто и сколько раз обновлять модель. `(1, "step")` и `1` соответственно — стандарт для максимального использования данных.

---

### Ключевые параметры, уникальные для TD3

Вот здесь и кроется вся суть и отличия TD3.

#### **`action_noise` (Шум действий)**

*   **Перевод:** Шум, добавляемый к действиям для исследования.
*   **Объяснение:** Это **критически важный параметр для TD3**. В отличие от SAC, политика которого стохастическая (сама по себе содержит случайность), политика TD3 — **детерминированная**. Она всегда выдает одно и то же действие для одного и того же состояния.
    *   Чтобы исследовать мир, TD3 *необходимо* искусственно добавлять шум к своим "уверенным" действиям. Без этого шума агент будет делать одно и то же и никогда не научится ничему новому.
*   **Аналогия:** Если SAC — это человек, который бросает дротики (есть естественный разброс), то TD3 — это робот, который всегда целится в одну точку. Чтобы робот исследовал доску, мы должны специально "трясти" его руку (`action_noise`).
*   **Рекомендация:** **Обязательно используйте шум.** Самый распространенный выбор — Гауссовский шум.
    ```python
    from stable_baselines3.common.noise import NormalActionNoise
    import numpy as np

    n_actions = env.action_space.shape[-1]
    action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))
    # ...и передайте это в конструктор TD3: action_noise=action_noise
    ```

#### **`policy_delay` (Задержка обновления политики)**

*   **Перевод:** Политика и целевые сети обновляются только каждый `policy_delay`-й шаг обучения.
*   **Объяснение:** **Это "D" (Delayed) в названии TD3.** Критики (Q-функции) обновляются на каждом шаге градиентного спуска, а вот Актер (политика) — реже.
*   **Зачем это нужно?** Оценки критиков вначале очень "шумные" и неточные. Если Актер будет обновляться на основе этих плохих оценок, он быстро научится чему-то не тому. Задержка дает критикам время "прийти в себя" и выдать более стабильную оценку, и только потом Актер использует эту оценку для своего обновления.
*   **Аналогия:** CEO (Актер) не принимает стратегических решений после каждого отчета от младшего аналитика (Критика). Он ждет, пока финансовый отдел соберет и несколько раз проверит данные (`policy_delay` отчетов), и только потом действует.
*   **Рекомендация:** `2` — это стандартное значение из оригинальной статьи, и оно отлично работает. Это означает, что Актер обновляется в два раза реже, чем Критики.

#### **`target_policy_noise` (Шум целевой политики)**

*   **Перевод:** Стандартное отклонение Гауссовского шума, добавляемого к целевому действию.
*   **Объяснение:** Это третья "фишка" TD3 — сглаживание цели. При обновлении Критиков мы смотрим на то, какое действие *совершил бы* Актер в следующем состоянии. TD3 добавляет к этому действию небольшой случайный шум.
*   **Зачем это нужно?** Это не дает Актеру "обманывать" Критиков, эксплуатируя очень узкие пики в оценке Q-функции. Добавление шума заставляет Q-функцию быть более "гладкой" в окрестностях хороших действий, что делает обучение более робастным.
*   **Аналогия:** Вы учите снайпера. Вместо того чтобы заставлять его целиться в один-единственный пиксель, вы слегка "качаете" мишень. Это заставляет его научиться целиться в небольшую область, а не в конкретную точку, что делает его навык более устойчивым к реальным условиям (ветер, дрожание рук).
*   **Рекомендация:** `0.2` — стандартное значение.

#### **`target_noise_clip` (Отсечение шума цели)**

*   **Перевод:** Ограничение для абсолютного значения шума сглаживания.
*   **Объяснение:** Это "ограничитель" для предыдущего параметра. Шум — это хорошо, но слишком сильный шум может сбить обучение с толку. Этот параметр говорит: "Добавляй шум, но не позволяй ему отклонять целевое действие слишком далеко".
*   **Рекоменда-ция:** `0.5` — стандартное значение.

---

### Краткое сравнение: TD3 vs SAC

| Характеристика          | TD3                                                                | SAC                                                                          |
| ----------------------- | ------------------------------------------------------------------ | ---------------------------------------------------------------------------- |
| **Природа Политики**    | Детерминированная (одно действие на состояние)                     | Стохастическая (распределение вероятностей по действиям)                      |
| **Источник Исследования** | Внешний **`action_noise`** (обязателен!)                             | Внутренняя энтропия (параметр **`ent_coef`**, обычно 'auto')                   |
| **Стабильность**        | Очень высокая, благодаря "тройной защите" (Twin, Delayed, Smoothing) | Высокая, но может быть чуть менее стабилен, чем TD3, на некоторых задачах. |
| **Эффективность данных** | Хорошая, но может требовать больше сэмплов, чем SAC, из-за менее эффективного исследования. | Обычно считается более сэмпл-эффективным благодаря исследованию через энтропию. |
| **Ключевые "ручки"**    | `action_noise`, `policy_delay`                                     | `ent_coef`                                                                   |

**Когда выбирать TD3?** Если вы столкнулись с проблемами стабильности в SAC, или если ваша задача не требует очень широкого и сложного исследования, TD3 может оказаться более надежным выбором. Это "рабочая лошадка" для многих задач робототехники.